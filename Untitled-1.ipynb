{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "\n",
    "# Specify the paths to your train and test datasets\n",
    "train_root_dir = r'D:\\my_pro\\train'\n",
    "test_root_dir = r'D:\\my_pro\\test'\n",
    "\n",
    "\n",
    "# Define custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(data_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Define dataset directory and transform (resize and convert to tensor)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fixed size\n",
    "    transforms.ToTensor()            # Convert PIL Image to tensor\n",
    "])\n",
    "\n",
    "# Create custom dataset instance\n",
    "dataset = CustomDataset(train_root_dir, transform=transform)\n",
    "\n",
    "# Initialize lists to store pixel values\n",
    "r_channel, g_channel, b_channel = [], [], []\n",
    "\n",
    "# Iterate over the dataset to collect pixel values\n",
    "for image in dataset:\n",
    "    # Extract pixel values for each channel (R, G, B)\n",
    "    r_channel.append(image[0].mean())  # Red channel\n",
    "    g_channel.append(image[1].mean())  # Green channel\n",
    "    b_channel.append(image[2].mean())  # Blue channel\n",
    "\n",
    "# Calculate mean and std across all pixel values\n",
    "mean = torch.mean(torch.tensor(r_channel)), torch.mean(torch.tensor(g_channel)), torch.mean(torch.tensor(b_channel))\n",
    "std = torch.std(torch.tensor(r_channel)), torch.std(torch.tensor(g_channel)), torch.std(torch.tensor(b_channel))\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation (std): {std}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation pipeline for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "# Load the datasets using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_root_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_root_dir, transform=transform)\n",
    "\n",
    "# Create data loaders for batching and shuffling\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define your CNN model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, stride=1,padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "\n",
    "    nn.Conv2d(32,64 , kernel_size=3, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(128, 512, kernel_size=3, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(512, 700, kernel_size=3, stride=1, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),    \n",
    "    \n",
    "  \n",
    "\n",
    "    nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(700, 256),  # Adjust the output size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    \n",
    "\n",
    "    nn.Linear(256, 1)\n",
    "        # Output size matches the number of classes (apple, tomato)\n",
    ")\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss for multi-class classification)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#\n",
    "print(\"iii\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        #print(labels.shape,labels.dtype)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        output_tensor = outputs.squeeze()\n",
    "\n",
    "       # Convert the tensor to float32 dtype\n",
    "        outputs = output_tensor.type(torch.float32)\n",
    "        #print(outputs.dtype,outputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        # Zero gradients, backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 55.67\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking during inference\n",
    "    for images, labels in test_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class index\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (correct_predictions / total_samples)*100\n",
    "\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_path = r'D:\\my_pro\\test\\tomatoes\\img_p3_34.jpeg'\n",
    "image = Image.open(image_path).convert('RGB')  # Ensure image is RGB format\n",
    "image = transform(image)\n",
    "image_tensor = image.unsqueeze(0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(image_tensor)\n",
    "    pred = torch.sigmoid(out)\n",
    "    print(pred)\n",
    "    pred = torch.round(pred).item()\n",
    "    print(pred)\n",
    "    if pred == 1:\n",
    "        print(\"tomatoes\")\n",
    "    if pred == 0:\n",
    "        print(\"apples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
